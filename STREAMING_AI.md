# ğŸŒŠ Streaming AI - Chain of Thoughts en Temps RÃ©el

## ğŸ¯ Objectif

Afficher le "chain of thoughts" de l'IA pendant que l'appel API Ã  ChatGPT se fait, au lieu d'attendre 10-15 secondes avec un Ã©cran figÃ©.

## âœ¨ FonctionnalitÃ©s ImplÃ©mentÃ©es

### Backend (`backend/src/routes/ai.routes.ts`)

1. **Streaming OpenAI activÃ©**
   - Ajout du paramÃ¨tre `stream: true` dans l'appel Ã  l'API OpenAI
   - Les rÃ©ponses arrivent mot par mot au lieu d'un bloc unique

2. **Server-Sent Events (SSE)**
   - Le backend transforme le stream OpenAI en Ã©vÃ©nements SSE
   - Format : `text/event-stream`
   - Connexion maintenue ouverte pendant toute la durÃ©e du stream

3. **Types d'Ã©vÃ©nements envoyÃ©s**
   ```typescript
   // Chunk de texte reÃ§u
   { type: 'chunk', content: 'mot ou phrase' }
   
   // Stream terminÃ© avec JSON complet
   { type: 'done', data: '{"title":"...","workflowText":"...","preferences":{...}}' }
   
   // Erreur survenue
   { type: 'error', error: 'message d\'erreur' }
   ```

4. **Validation du JSON Ã  la fin**
   - Le texte est accumulÃ© pendant le stream
   - Ã€ la fin, on valide que c'est un JSON valide avec les clÃ©s requises
   - Envoi d'un Ã©vÃ©nement `done` avec le JSON complet

### Frontend (`frontend/src/.../polygon-sidebar-designer-view.tsx`)

1. **Gestion des messages**
   - State `messages` pour stocker l'historique de conversation
   - Type `Message` avec `role`, `content`, `isStreaming`

2. **Lecture du stream**
   - Utilisation de `fetch()` avec `response.body.getReader()`
   - DÃ©codage progressif des chunks SSE
   - Mise Ã  jour du message en temps rÃ©el

3. **Affichage progressif**
   - Chaque chunk reÃ§u est ajoutÃ© au contenu du message
   - Animation de curseur clignotant pendant le streaming
   - Auto-scroll vers le bas quand de nouveaux messages arrivent

4. **Mise Ã  jour du workflow**
   - Une fois le JSON complet reÃ§u (Ã©vÃ©nement `done`)
   - Parsing et mise Ã  jour du workflow dans le store
   - Gestion des erreurs avec rollback

### UI/UX (`polygon-sidebar-designer-view.module.scss`)

1. **Styles des messages**
   - Messages utilisateur : fond violet clair, alignÃ©s Ã  droite
   - Messages assistant : fond gris clair, alignÃ©s Ã  gauche
   - IcÃ´nes distinctes (User vs Bot)

2. **Animations**
   - `fadeIn` : apparition douce des messages
   - `blink` : curseur clignotant pendant le streaming

3. **Responsive**
   - Texte avec `pre-wrap` pour prÃ©server les sauts de ligne
   - `word-wrap` pour Ã©viter les dÃ©bordements

## ğŸ”„ Flux de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚
â”‚  (Textarea) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ User envoie "CrÃ©er un workflow..."
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  handleSendMessage  â”‚
â”‚  - CrÃ©e userMessage â”‚
â”‚  - CrÃ©e assistantMessage (vide, isStreaming: true)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /api/ai/chat
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (Hono)            â”‚
â”‚  - ReÃ§oit message          â”‚
â”‚  - Appelle OpenAI (stream: true)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Stream SSE
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI API                â”‚
â”‚  - GÃ©nÃ¨re JSON progressivement
â”‚  - Envoie chunks          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Chunks OpenAI
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (ReadableStream)  â”‚
â”‚  - ReÃ§oit chunks OpenAI    â”‚
â”‚  - Transforme en SSE       â”‚
â”‚  - Envoie { type: 'chunk', content: '...' }
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ SSE Events
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (Reader)         â”‚
â”‚  - Lit Ã©vÃ©nements SSE      â”‚
â”‚  - Accumule contenu        â”‚
â”‚  - Met Ã  jour UI en temps rÃ©el
â”‚  - Affiche "chain of thoughts" âš¡
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Stream terminÃ©
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend                   â”‚
â”‚  - Valide JSON complet     â”‚
â”‚  - Envoie { type: 'done', data: JSON }
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ JSON complet
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend                  â”‚
â”‚  - Parse JSON              â”‚
â”‚  - Met Ã  jour workflow     â”‚
â”‚  - DÃ©sactive isStreaming   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¨ ExpÃ©rience Utilisateur

### Avant (sans streaming)
```
User: "CrÃ©er un workflow de validation"
        â†“
[Spinner pendant 10-15 secondes] â³
        â†“
âœ… Workflow crÃ©Ã© !
```

### AprÃ¨s (avec streaming) â­
```
User: "CrÃ©er un workflow de validation"
        â†“
Bot: "{"  [< 1 seconde]
Bot: "{\"title"  [streaming...]
Bot: "{\"title\":\"Validation"  [streaming...]
Bot: "{\"title\":\"Validation de donnÃ©es\",\"workflow"  [streaming...]
... [l'utilisateur voit le texte s'Ã©crire en temps rÃ©el]
        â†“
âœ… Workflow crÃ©Ã© ! [aprÃ¨s 10-15 secondes mais avec feedback visuel]
```

## ğŸ”§ Points Techniques Importants

### 1. Double Validation du JSON

**ProblÃ¨me** : On stream du texte brut mais on a besoin de JSON valide Ã  la fin.

**Solution** :
- Le texte est affichÃ© progressivement tel quel (chain of thoughts visible)
- Le JSON est accumulÃ© dans `accumulatedContent`
- Ã€ la fin, on parse le JSON complet pour mettre Ã  jour le workflow
- Si le parsing Ã©choue, on affiche une erreur

### 2. Gestion des Erreurs

**Types d'erreurs gÃ©rÃ©es** :
- Erreur rÃ©seau (fetch failed)
- Erreur OpenAI (API key invalide, quota dÃ©passÃ©)
- Erreur parsing JSON (rÃ©ponse malformÃ©e)
- Interruption du stream

**Try-catch** :
```typescript
try {
  // Lecture du stream
} catch (error) {
  // Met Ã  jour le message assistant avec l'erreur
  setMessages(prev => 
    prev.map(msg => 
      msg.id === assistantMessageId 
        ? { ...msg, content: `âŒ Erreur: ...`, isStreaming: false }
        : msg
    )
  );
}
```

### 3. Performance

- Pas de re-render inutile : on met Ã  jour uniquement le message concernÃ©
- Auto-scroll optimisÃ© avec `behavior: 'smooth'`
- DÃ©codage progressif avec `TextDecoder({ stream: true })`

## ğŸš€ Comment Tester

1. **DÃ©marrer le backend**
   ```bash
   cd backend
   bun run dev
   ```

2. **DÃ©marrer le frontend**
   ```bash
   cd frontend
   bun run dev
   ```

3. **Ouvrir l'app et aller dans un workflow**

4. **Cliquer sur l'icÃ´ne AI (Bot) dans la sidebar**

5. **Taper un message et observer le streaming** ğŸŒŠ
   - "CrÃ©er un workflow de validation de formulaire"
   - Observer le texte apparaÃ®tre progressivement
   - Observer le curseur clignotant pendant le streaming

## ğŸ“ AmÃ©liorations Futures Possibles

1. **Markdown Rendering**
   - Parser le markdown dans les rÃ©ponses (gras, listes, code blocks)

2. **Code Syntax Highlighting**
   - Si l'IA envoie du code XML/JSON, le colorer

3. **Stop Button**
   - Permettre d'annuler le stream en cours
   - Utiliser `AbortController`

4. **Historique Persistant**
   - Sauvegarder les conversations dans localStorage
   - Reprendre la conversation oÃ¹ on l'a laissÃ©e

5. **Multi-turn Conversation**
   - Envoyer l'historique complet Ã  l'IA
   - Context awareness pour les questions de suivi

6. **Retry Mechanism**
   - Bouton "RÃ©essayer" en cas d'erreur
   - Reconnexion automatique si le stream Ã©choue

## âš ï¸ Notes Importantes

- L'API OpenAI doit supporter le streaming (gpt-4o-mini âœ…)
- Le format `response_format: { type: "json_object" }` fonctionne avec le streaming
- Les CORS doivent Ãªtre correctement configurÃ©s sur le backend
- Le streaming consomme autant de tokens qu'une rÃ©ponse classique

## ğŸ‰ RÃ©sultat

Au lieu d'attendre 10-15 secondes dans le vide, l'utilisateur voit maintenant le "raisonnement" de l'IA se construire en temps rÃ©el, crÃ©ant une expÃ©rience beaucoup plus engageante et transparente ! ğŸš€

